{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Modeling\n",
    "\n",
    "Evaluates LSTM with various combinations of hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from src.features.smiles import SmilesTokenizer\n",
    "from src.models.lstm_model import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load('data/interim/smiles_train.npy')\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = SmilesTokenizer()\n",
    "vocab_size = st.table_len\n",
    "\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "X = dataset[:, :-1]\n",
    "labels = dataset[:, -1:]\n",
    "\n",
    "VAL_SPLIT = .10\n",
    "\n",
    "y = tf.keras.utils.to_categorical(labels, num_classes=vocab_size)\n",
    "\n",
    "X_train, X_test = X[:int(X.shape[0] * (1 - VAL_SPLIT))], X[int(X.shape[0] * (1 - VAL_SPLIT)):]\n",
    "y_train, y_test = y[:int(y.shape[0] * (1 - VAL_SPLIT))], y[int(y.shape[0] * (1 - VAL_SPLIT)):]\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1]))\n",
    "\n",
    "max_length = X_train.shape[1]\n",
    "train_size = X_train.shape[0]\n",
    "\n",
    "print('Vocabulary size: ', vocab_size)\n",
    "print('Max length: ', max_length)\n",
    "print('Train size: ', train_size)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([128, 256]))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.1, 0.2))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['nadam', 'adam', 'rmsprop']))\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_OPTIMIZER],\n",
    "        metrics=[hp.Metric('mae', display_name='MAE'),\n",
    "                 hp.Metric('acc', display_name='Accuracy'),\n",
    "                ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir='logs')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "reduce_lr_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss')\n",
    "\n",
    "checkpoint_filepath = 'models/checkpoint'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_acc',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "weight_init = RandomNormal(mean=0.0, stddev=0.05, seed=71)\n",
    "\n",
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)\n",
    "        \n",
    "        model = build_model(vocab_size, hparams[HP_NUM_UNITS], hparams[HP_DROPOUT], hparams[HP_OPTIMIZER])\n",
    "        \n",
    "        model.fit(X_train, y_train, validation_data=(X_test, y_test), shuffle=False, epochs=NUM_EPOCHS, batch_size=1000, callbacks=[tensorboard, es, model_checkpoint_callback, reduce_lr_on_plateau])\n",
    "    \n",
    "        print(model.summary())\n",
    "    \n",
    "        scores = model.evaluate(X_test, y_test)\n",
    "        \n",
    "        tf.summary.scalar('mae', scores[0], step=1)\n",
    "        tf.summary.scalar('acc', scores[1], step=1)\n",
    "\n",
    "session_num = 0\n",
    "\n",
    "for num_units in HP_NUM_UNITS.domain.values:\n",
    "    for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n",
    "        for optimizer in HP_OPTIMIZER.domain.values:\n",
    "            hparams = {\n",
    "                HP_NUM_UNITS: num_units,\n",
    "                HP_DROPOUT: dropout_rate,\n",
    "                HP_OPTIMIZER: optimizer,\n",
    "            }\n",
    "            run_name = \"run-%d\" % session_num\n",
    "            print('--- Starting trial: %s' % run_name)\n",
    "            print({h.name: hparams[h] for h in hparams})\n",
    "            run('logs/hparam_tuning/' + run_name, hparams)\n",
    "            session_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir logs/hparam_tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
