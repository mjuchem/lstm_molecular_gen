{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate\n",
    "\n",
    "Load trained weights, synthesize new sequences, save SMILES to a plain text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from rdkit import Chem\n",
    "\n",
    "from src.features.smiles import SmilesTokenizer, cleanup_list_smiles, encode_list_smiles\n",
    "from src.models.lstm_model import build_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model and load trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = SmilesTokenizer()\n",
    "vocab_size = st.table_len\n",
    "\n",
    "model = build_model(vocab_size, 128, .1, 'nadam')\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.load_weights('models/model_nadam_128_100epochs_1000batch.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load('data/interim/smiles_train.npy')\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[:, :-1]\n",
    "labels = dataset[:, -1:]\n",
    "y = tf.keras.utils.to_categorical(labels, num_classes=vocab_size)\n",
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype('int')\n",
    "y = tf.argmax(y, axis=1).numpy().astype('int')\n",
    "y_pred = tf.argmax(y_pred, axis=1).numpy().astype('int')\n",
    "\n",
    "print(X.shape)\n",
    "print(labels.shape)\n",
    "print(y.shape)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesize sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = []\n",
    "for i in range(len(X)):\n",
    "    s = st.embeddings_to_smiles(X[i]) + st.embeddings_to_smiles(y[i])\n",
    "    smiles.append(s)\n",
    "\n",
    "for i in range(len(X)):\n",
    "    s = st.embeddings_to_smiles(X[i]) + st.embeddings_to_smiles(y_pred[i])\n",
    "    smiles.append(s)\n",
    "\n",
    "print(\"# SMILES:\", len(smiles))\n",
    "\n",
    "smiles = list(set(smiles))\n",
    "\n",
    "print(\"# SMILES (de-duplicated):\")\n",
    "\n",
    "valid_smiles = cleanup_list_smiles(smiles)\n",
    "\n",
    "print(\"# SMILES (validated):\", valid_smiles)\n",
    "\n",
    "print('Generated:', len(smiles))\n",
    "print('Valid:', len(valid_smiles))\n",
    "        \n",
    "with open('data/processed/valid_smiles.smi', 'w') as f:\n",
    "    for s in valid_smiles:\n",
    "        f.write(\"%s\\n\" % s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
