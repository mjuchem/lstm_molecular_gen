{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coronavirus\n",
    "\n",
    "Self-contained LSTM to train using Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmilesTokenizer(object):\n",
    "    def __init__(self):\n",
    "        atoms = ['Li', 'Na', 'Al', 'Si', 'Cl', 'Sc', 'Zn', 'As', 'Se', 'Br', 'Sn', 'Te', 'Cn', 'H', 'B', 'C', 'N', 'O', 'F', 'P', 'S', 'K', 'V', 'I', ]\n",
    "        special = ['(', ')', '[', ']', '=', '#', '%', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '+', '-', 'se', 'te', 'c', 'n', 'o', 's']\n",
    "        padding = ['G', 'A', 'E']\n",
    "\n",
    "        self.table = sorted(atoms, key=len, reverse=True) + special + padding\n",
    "        self.table_len = len(self.table)\n",
    "\n",
    "        self.table_2_chars = list(filter(lambda x: len(x) == 2, self.table))\n",
    "        self.table_1_chars = list(filter(lambda x: len(x) == 1, self.table))\n",
    "\n",
    "        self.one_hot_dict = {}\n",
    "        for i, symbol in enumerate(self.table):\n",
    "            vec = np.zeros(self.table_len, dtype=np.float32)\n",
    "            vec[i] = 1\n",
    "            self.one_hot_dict[symbol] = vec\n",
    "\n",
    "    def tokenize(self, smiles):\n",
    "\n",
    "        smiles = smiles + ' '\n",
    "        \n",
    "        N = len(smiles)\n",
    "        \n",
    "        token = []\n",
    "        i = 0\n",
    "        \n",
    "        while (i < N):\n",
    "            c1 = smiles[i]\n",
    "            c2 = smiles[i : i+2]\n",
    "            \n",
    "            if (c2 in self.table_2_chars):\n",
    "                token.append(c2)\n",
    "                i = i + 1\n",
    "                continue\n",
    "                \n",
    "            if (c1 in self.table_1_chars):\n",
    "                token.append(c1)\n",
    "                i = i + 1\n",
    "                continue\n",
    "                \n",
    "            i = i + 1\n",
    "\n",
    "        return token\n",
    "\n",
    "    def one_hot_encode(self, tokenized_smiles):\n",
    "        result = np.array(\n",
    "            [self.one_hot_dict[symbol] for symbol in tokenized_smiles],\n",
    "            dtype=np.float32)\n",
    "        result = result.reshape(1, result.shape[0], result.shape[1])\n",
    "        return result\n",
    "\n",
    "    def embeddings(self, tokenized_smiles):\n",
    "        result = [self.table.index(symbol) for symbol in tokenized_smiles]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"../input/molecule-candidates-coronavirus/dataset.smi\", names=[\"smiles\"])\n",
    "hiv_inhibitors = pd.read_csv(\"../input/molecule-candidates-coronavirus/hiv_inhibitors.smi\", names=[\"smiles\"])\n",
    "hiv_inhibitors = pd.read_csv(\"../input/molecule-candidates-coronavirus/hiv_inhibitors.smi\", names=[\"smiles\"])\n",
    "known_TRPM8_inhibitors = pd.read_csv(\"../input/molecule-candidates-coronavirus/known_TRPM8-inhibitors.smi\", names=[\"smiles\"])\n",
    "manual_testing = pd.read_csv(\"../input/molecule-candidates-coronavirus/manual_testing.smi\", names=[\"smiles\"])\n",
    "\n",
    "df = pd.concat([dataset, hiv_inhibitors, known_TRPM8_inhibitors])\n",
    "\n",
    "df = df.loc[\n",
    "    (df['smiles'].str.len() <= 200)\n",
    "]\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = SmilesTokenizer()\n",
    "\n",
    "encoded_smiles = []\n",
    "\n",
    "for s in df['smiles']:\n",
    "    t = st.tokenize(s)\n",
    "    e = st.embeddings(t)\n",
    "    encoded_smiles.append(e)\n",
    "\n",
    "dataset = pad_sequences(encoded_smiles, maxlen=None, dtype='float32', padding='pre', value=0.0)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('tmp/smiles_train.npy', dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = SmilesTokenizer()\n",
    "vocab_size = st.table_len\n",
    "\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "X = dataset[:, :-1]\n",
    "labels = dataset[:, -1:]\n",
    "\n",
    "VAL_SPLIT = .10\n",
    "\n",
    "y = tf.keras.utils.to_categorical(labels, num_classes=vocab_size)\n",
    "\n",
    "X_train, X_test = X[:int(X.shape[0] * (1 - VAL_SPLIT))], X[int(X.shape[0] * (1 - VAL_SPLIT)):]\n",
    "y_train, y_test = y[:int(y.shape[0] * (1 - VAL_SPLIT))], y[int(y.shape[0] * (1 - VAL_SPLIT)):]\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1]))\n",
    "\n",
    "max_length = X_train.shape[1]\n",
    "train_size = X_train.shape[0]\n",
    "\n",
    "print('Vocabulary size: ', vocab_size)\n",
    "print('Max length: ', max_length)\n",
    "print('Train size: ', train_size)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "\n",
    "weight_init = RandomNormal(mean=0.0, stddev=0.05, seed=71)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 128, input_length=max_length),\n",
    "    tf.keras.layers.LSTM(128, return_sequences=True, kernel_initializer=weight_init, dropout=.1),\n",
    "    tf.keras.layers.LSTM(128, kernel_initializer=weight_init, dropout=.1),\n",
    "    tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
    "])\n",
    "    \n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"nadam\", metrics=['mae', 'acc'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), shuffle=False, epochs=NUM_EPOCHS, batch_size=5000, callbacks=[es])\n",
    "\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(history)\n",
    "print(scores)\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "model.save('model_nadam_128_100epochs_1000batch.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
